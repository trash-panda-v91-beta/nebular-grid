---
# yaml-language-server: $schema=https://raw.githubusercontent.com/siderolabs/talos/refs/heads/release-1.10/website/content/v1.10/schemas/config.schema.json
version: v1alpha1
machine:
  ca:
    crt: "op://NebularGrid/6i7sewsv3lus3qxpwddknc7jem/machine ca crt"
    {% if ENV.IS_CONTROLPLANE %}
    key: "op://NebularGrid/6i7sewsv3lus3qxpwddknc7jem/machine ca key"
    {% endif %}
  features:
    apidCheckExtKeyUsage: true
    diskQuotaSupport: true
    hostDNS:
      enabled: true
      forwardKubeDNSToHost: false
      resolveMemberNames: true
    kubePrism:
      enabled: true
      port: 7445
    {% if ENV.IS_CONTROLPLANE %}
    kubernetesTalosAPIAccess:
      enabled: true
      allowedRoles:
        - os:admin
      allowedKubernetesNamespaces:
        - actions-runner-system
        - system-upgrade
    {% endif %}
    rbac: true
    stableHostname: true
  files:
    - op: create
      path: /etc/cri/conf.d/20-customization.part
      content: |-
        [plugins]
          [plugins."io.containerd.grpc.v1.cri"]
            enable_unprivileged_ports = true
            enable_unprivileged_icmp = true
        [plugins."io.containerd.cri.v1.images"]
          discard_unpacked_layers = false
        [plugins."io.containerd.cri.v1.runtime"]
          cdi_spec_dirs = ["/var/cdi/static", "/var/cdi/dynamic"]
          device_ownership_from_security_context = true
    - op: overwrite
      path: /etc/nfsmount.conf
      permissions: 0o644
      content: |
        [ NFSMount_Global_Options ]
        nfsvers=4.2
        hard=True
        noatime=True
        nconnect=16
  install:
    image: factory.talos.dev/metal-installer/a30c16a32db3c99cb35f22401fad96807f80896dfc86aa4ec716ed6b4aff09de:v1.11.5
  kernel:
    modules:
      - name: nbd
      - name: thunderbolt
      - name: thunderbolt_net
  kubelet:
    defaultRuntimeSeccompProfileEnabled: true
    disableManifestsDirectory: true
    extraConfig:
      featureGates:
        ResourceHealthStatus: true
      maxPods: 150
      serializeImagePulls: false
    image: ghcr.io/siderolabs/kubelet:v1.34.2
    nodeIP:
      validSubnets:
        - 10.1.1.0/24
  network:
    disableSearchDomain: true
    interfaces:
      - interface: bond0
        bond:
          deviceSelectors:
            - hardwareAddr: 58:47:ca:79:64:84
              driver: i40e
            - hardwareAddr: 38:05:25:31:7f:f2
              driver: i40e
            - hardwareAddr: 58:47:ca:7a:bd:ae
              driver: i40e
          mode: active-backup
        dhcp: true
        mtu: 9000
        vlans:
          - vlanId: 303
            dhcp: false
            mtu: 1500
          - vlanId: 305
            dhcp: false
            mtu: 1500
  sysctls:
    fs.inotify.max_user_instances: "8192"
    fs.inotify.max_user_watches: "1048576"
    net.core.rmem_max: "67108864"
    net.core.wmem_max: "67108864"
    net.ipv4.neigh.default.gc_thresh1: "4096"
    net.ipv4.neigh.default.gc_thresh2: "8192"
    net.ipv4.neigh.default.gc_thresh3: "16384"
    net.ipv4.tcp_fastopen: "3"
    user.max_user_namespaces: "11255"
  time:
    disabled: false
    servers:
      - 10.1.1.12
  token: "op://NebularGrid/6i7sewsv3lus3qxpwddknc7jem/machine token"
cluster:
  {% if ENV.IS_CONTROLPLANE %}
  aggregatorCA:
    crt: "op://NebularGrid/6i7sewsv3lus3qxpwddknc7jem/aggregator ca crt"
    key: "op://NebularGrid/6i7sewsv3lus3qxpwddknc7jem/aggregator ca key"
  allowSchedulingOnControlPlanes: true
  apiServer:
    image: registry.k8s.io/kube-apiserver:v1.34.2
    extraArgs:
      enable-aggregator-routing: "true"
      feature-gates: MutatingAdmissionPolicy=true,ResourceHealthStatus=true
      runtime-config: admissionregistration.k8s.io/v1beta1=true
    certSANs:
      - 127.0.0.1 # KubePrism
      - 10.1.6.50
      - psb.internal
    disablePodSecurityPolicy: true
  controllerManager:
    image: registry.k8s.io/kube-controller-manager:v1.34.2
    extraArgs:
      bind-address: 0.0.0.0
  coreDNS:
    disabled: true
  etcd:
    advertisedSubnets:
      - 10.1.1.0/24
    ca:
      crt: "op://NebularGrid/6i7sewsv3lus3qxpwddknc7jem/etcd ca crt"
      key: "op://NebularGrid/6i7sewsv3lus3qxpwddknc7jem/etcd ca key"
    extraArgs:
      listen-metrics-urls: http://0.0.0.0:2381
  proxy:
    disabled: true
    image: registry.k8s.io/kube-proxy:v1.34.2
  scheduler:
    config:
      apiVersion: kubescheduler.config.k8s.io/v1
      kind: KubeSchedulerConfiguration
      profiles:
        - pluginConfig:
            - args:
                defaultConstraints:
                  - maxSkew: 1
                    topologyKey: kubernetes.io/hostname
                    whenUnsatisfiable: ScheduleAnyway
                defaultingType: List
              name: PodTopologySpread
          plugins:
            score:
              disabled:
                - name: ImageLocality
          schedulerName: default-scheduler
    extraArgs:
      bind-address: 0.0.0.0
    image: registry.k8s.io/kube-scheduler:v1.34.2
  secretboxEncryptionSecret: "op://NebularGrid/6i7sewsv3lus3qxpwddknc7jem/secretbox encryption secret"
  serviceAccount:
    key: "op://NebularGrid/6i7sewsv3lus3qxpwddknc7jem/service account"
  {% endif %}
  ca:
    crt: "op://NebularGrid/6i7sewsv3lus3qxpwddknc7jem/ca crt"
    {% if ENV.IS_CONTROLPLANE %}
    key: "op://NebularGrid/6i7sewsv3lus3qxpwddknc7jem/ca key"
    {% endif %}
  controlPlane:
    endpoint: https://10.1.6.50:6443
  clusterName: psb
  discovery:
    enabled: true
    registries:
      kubernetes: { disabled: true }
      service: { disabled: false }
  id: "op://NebularGrid/6i7sewsv3lus3qxpwddknc7jem/cluster id"
  network:
    cni:
      name: none
    dnsDomain: cluster.local
    podSubnets:
      - 10.244.0.0/16
    serviceSubnets:
      - 10.96.0.0/12
  secret: "op://NebularGrid/6i7sewsv3lus3qxpwddknc7jem/cluster secret"
  token: "op://NebularGrid/6i7sewsv3lus3qxpwddknc7jem/cluster token"
---
apiVersion: v1alpha1
kind: WatchdogTimerConfig
device: /dev/watchdog0
timeout: 5m
---
apiVersion: v1alpha1
kind: VolumeConfig
name: EPHEMERAL
provisioning:
  diskSelector:
    match: system_disk
  maxSize: 512GiB
---
apiVersion: v1alpha1
kind: UserVolumeConfig
name: local-hostpath
provisioning:
  diskSelector:
    match: system_disk
  maxSize: 256GiB
